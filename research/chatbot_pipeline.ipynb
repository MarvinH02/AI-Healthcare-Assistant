{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4e2efc1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain.document_loaders import PyPDFLoader, DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain_openai import OpenAI\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5d904c79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load environment variables\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ba347d21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\marre\\\\Desktop\\\\Kth\\\\ID1214\\\\AI-Healthcare-Assistant\\\\research'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "57aadb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set API keys from environment variables\n",
    "os.environ[\"PINECONE_API_KEY\"] = os.getenv(\"PINECONE_API_KEY\")\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ade19efe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\marre\\\\Desktop\\\\Kth\\\\ID1214\\\\Medical-Chatbot\\\\research'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755a520b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total text chunks: 28923\n"
     ]
    }
   ],
   "source": [
    "# Function to extract text from PDF in a specific directory.\n",
    "def extract_text_from_pdfs(directory_path):\n",
    "    # Document processing - automated extraction of text from PDFs\n",
    "    pdf_loader = DirectoryLoader(directory_path, glob=\"*.pdf\", loader_cls=PyPDFLoader)\n",
    "    documents = pdf_loader.load()\n",
    "    return documents\n",
    "\n",
    "# Function that splits the document into smaller chunks using RecursiveCharacterTextSplitter from langchain.text_splitter\n",
    "def split_documents_into_chunks(documents, chunk_size=700, chunk_overlap=30):\n",
    "    # Text-chunking is done here, break large texts into semantic units for processing.\n",
    "    # this Improves the context retrieval and helps the management of context window limitations\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    # Split our document into chunks\n",
    "    return splitter.split_documents(documents)\n",
    "\n",
    "# Load PDF documents from the data directory\n",
    "raw_documents = extract_text_from_pdfs(\"../Data\")\n",
    "# Split documents into smaller chunks for better processing\n",
    "text_chunks = split_documents_into_chunks(raw_documents)\n",
    "print(f\"Total text chunks: {len(text_chunks)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731db680",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the embedding model\n",
    "def initialize_embeddings():\n",
    "    # Technique used to convert text into a numerical vector representation\n",
    "    # Use huggingface's sentence transformer to create dense vector representation.\n",
    "    return HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "embedding_model = initialize_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84cf4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone.grpc import PineconeGRPC as Pinecone\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "import os\n",
    "\n",
    "pinecone_client = Pinecone(api_key=os.getenv(\"PINECONE_API_KEY\"))\n",
    "\n",
    "index_name = \"ai-healthcare-assistant\"\n",
    "\n",
    "# Create a pinecone index.\n",
    "# Vector database, will be utilised for similarity searching.\n",
    "pinecone_client.create_index(\n",
    "    name=index_name,\n",
    "    dimension=384, # The embedding vector has a dimension of 384.\n",
    "    metric=\"cosine\", # The metric is picked as cosine because it checks the similarity between two vectors.\n",
    "    spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f9d7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_pinecone import PineconeVectorStore\n",
    "\n",
    "# Create a vector store from the text chunks and embeddings\n",
    "# Vector indexing, organize them for efficient similarity search.\n",
    "vector_store = PineconeVectorStore.from_documents(\n",
    "    documents=text_chunks, # The chunks that will be stored\n",
    "    index_name=index_name, # The name of the pinecone index\n",
    "    embedding=embedding_model # The embedding model to use\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6eb6175",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a retriever from the vector store.\n",
    "# Info retrieval is done here, find the relevant documents based on similarity.\n",
    "# Return the 3 most similar documents.\n",
    "retriever = vector_store.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea0ba26",
   "metadata": {},
   "outputs": [],
   "source": [
    "language_model = OpenAI(temperature=0.4, max_tokens=500)\n",
    "\n",
    "template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", (\n",
    "        \"You are a knowledgable AI assistant. \"\n",
    "        \"Your task is to answer the user's question using only the information provided in the context\"\n",
    "        \"If the answer is not found in the context, tell the user 'I can not make sure what your disease is exactly.' \"\n",
    "        \"Do NOT make up information or provide guesses.\"\n",
    "        \"Keep responses limited to 3 clear and informative sentences, using plain language.\\n\\n{context}\"\n",
    "    )),\n",
    "    (\"human\", \"{input}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6388df4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a chain that combines the retrieved documents with the user query.\n",
    "# Take the retrieved content into the promp context.\n",
    "qa_chain = create_stuff_documents_chain(language_model, template)\n",
    "\n",
    "# Create a Retrieval-Augmented Generation pipeline \n",
    "# It works by combining retrival and generation.\n",
    "# It ensures the LLM uses domain specific knowledge. \n",
    "rag_pipeline = create_retrieval_chain(retriever, qa_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061446f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_query = \"What is AIDS?\"\n",
    "result = rag_pipeline.invoke({\"input\": user_query})\n",
    "print(result[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1496d730",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_query = \"I have the following symptoms, what is my disease? Shortness of breath, a high temprature, chest pain, an aching body, loss of apetite, a cough, making wheezing noises when I breathe,\"\n",
    "result = rag_pipeline.invoke({\"input\": user_query})\n",
    "print(result[\"answer\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
